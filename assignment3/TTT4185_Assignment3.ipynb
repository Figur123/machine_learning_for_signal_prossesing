{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TTT4185 Machine learning for Speech technology"
      ],
      "metadata": {
        "id": "G5UcVMluQy74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Part A: Classification\n",
        "\n",
        "This assignment assumes that the student has reviewed the material on deep neural networks.\n",
        "\n",
        "This assignment will use the [PyTorch library](https://pytorch.org/) to create neural networks.\n",
        "\n",
        "We will be using the same, small database of phonemes from assignment 2. Each phoneme is represented by the four first formant positions (\"F1\"-\"F4\") and their corresponding bandwidths (\"B1\"-\"B4\"). All numbers are in kHz. In addition, the speaker ID and the gender of the speaker are given for each phoneme.\n",
        "\n",
        "The first few cells of this notebook contain example code to load and extract data, setup a simple network and train a deep neural network for classification.\n"
      ],
      "metadata": {
        "id": "2BS2MdV6Bdc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch data\n",
        "\n",
        "This block will fetch the files with the formant data and prepare the data frames"
      ],
      "metadata": {
        "id": "y-GfrbLgROG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this will fetch the train.csv and test.csv so you do not need to manually upload them. Just run this block for each session\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "! wget https://folk.ntnu.no/plparson/ttt4185_data/Train.csv\n",
        "! wget https://folk.ntnu.no/plparson/ttt4185_data/Test.csv\n",
        "\n",
        "print(\"Keeping ['ae', 'ey', 'ux']\")\n",
        "\n",
        "train = pd.read_csv('Train.csv')\n",
        "train = train[train['Phoneme'].isin(['ae', 'ey', 'ux'])]\n",
        "train.to_pickle('Train.pkl')\n",
        "\n",
        "test = pd.read_csv('Test.csv')\n",
        "test = test[test['Phoneme'].isin(['ae', 'ey', 'ux'])]\n",
        "test, validation = train_test_split(test, train_size=0.5, random_state=42)\n",
        "test.to_pickle('Test.pkl')\n",
        "validation.to_pickle('Validation.pkl')\n",
        "\n",
        "print('Done!')"
      ],
      "metadata": {
        "id": "EnQzyYRkyA6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up datasets (example)"
      ],
      "metadata": {
        "id": "eNXEzJ6kBgxw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-2S1jKO8BcT1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class CustomFormantDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        annotations_file,\n",
        "        use_features = ['F1', 'F2'],\n",
        "        phoneme_column = 'Phoneme',\n",
        "        norm_data_to_mean = True\n",
        "    ):\n",
        "        if annotations_file[-3:] == 'pkl':\n",
        "          annotations_df = pd.read_pickle(annotations_file)\n",
        "        else:\n",
        "          # NOTE: assuming CSV. Will throw error if another file type provided\n",
        "          annotations_df = pd.read_csv(annotations_file)\n",
        "        use_features = [list(annotations_df.columns).index(feat) for feat in use_features]\n",
        "        # normalize the values\n",
        "        scaler = StandardScaler()\n",
        "        self.X_scaled = scaler.fit_transform(annotations_df.iloc[:, use_features])\n",
        "        # encode our lables from human friendly to integers\n",
        "        phoneme_column = list(annotations_df.columns).index(phoneme_column)\n",
        "        label_encoder = LabelEncoder()\n",
        "        self.integer_encoded = label_encoder.fit_transform(annotations_df.iloc[:, phoneme_column])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_scaled)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.integer_encoded[idx]\n",
        "        data = torch.from_numpy(self.X_scaled[idx]).float()\n",
        "        return data, label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a model (example)\n",
        "\n"
      ],
      "metadata": {
        "id": "idWFAQm3Bi0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 3), # output is 3 because that's how many phoneme labels we have\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, verbose=False):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if verbose:\n",
        "          if batch % 100 == 0:\n",
        "              loss, current = loss.item(), (batch + 1) * len(X)\n",
        "              print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn, verbose=False):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    if verbose:\n",
        "      print(f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct, test_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "EipkcTHiBpea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "### Set up the data\n",
        "use_features = ['F1', 'F2']\n",
        "\n",
        "train_dataset = CustomFormantDataset('/Train.pkl', use_features = use_features)\n",
        "validation_dataset = CustomFormantDataset('/Validation.pkl', use_features = use_features)\n",
        "test_dataset = CustomFormantDataset('/Test.pkl', use_features = use_features)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "### Set up the model\n",
        "epochs = 100\n",
        "\n",
        "model = NeuralNetwork(input_dim=len(use_features)).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "validation_results = []\n",
        "\n",
        "### Do the training\n",
        "for t in tqdm(range(epochs), desc='Epochs'):\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    train_corr, train_loss = test(train_dataloader, model, loss_fn)\n",
        "    val_corr, val_loss = test(validation_dataloader, model, loss_fn)\n",
        "    validation_results.append([t+1, 'accuracy', train_corr, 'train'])\n",
        "    validation_results.append([t+1, 'loss', train_loss, 'train'])\n",
        "    validation_results.append([t+1, 'accuracy', val_corr, 'val'])\n",
        "    validation_results.append([t+1, 'loss', val_loss, 'val'])\n",
        "\n",
        "### Plot training and validation accuracy and lost across epochs\n",
        "sns.lineplot(\n",
        "    pd.DataFrame(validation_results, columns=['epoch', 'metric', 'value', 'data_type']),\n",
        "    x = 'epoch',\n",
        "    y = 'value',\n",
        "    hue = 'metric',\n",
        "    style = 'data_type'\n",
        ")\n",
        "plt.title('Features: {}'.format(use_features))\n",
        "plt.show()\n",
        "\n",
        "### Test network on wholly unseen data\n",
        "test_correct, test_loss = test(test_dataloader, model, loss_fn)\n",
        "print(f\"Test Error: \\n Accuracy: {(100*test_correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
      ],
      "metadata": {
        "id": "vUWR-nr7B3va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1\n",
        "\n",
        "Increase the number of features to include \"F3\" and \"F4\" and rerun the experiments"
      ],
      "metadata": {
        "id": "2YCS7jNJzNvt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PV3eXvTNzNKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2\n",
        "\n",
        "Try also adding the bandwidths (\"B1\"-\"B4\")."
      ],
      "metadata": {
        "id": "fZJ8ALZ8zYOz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QjlvKoaMzdf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3\n",
        "\n",
        "Change the number of nodes in the hidden layer and see how the results change."
      ],
      "metadata": {
        "id": "RIAgunUZzd96"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rCuCPkeXziOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4\n",
        "\n",
        "Add multiple layers to the network and observe the results"
      ],
      "metadata": {
        "id": "xaT7mJH6zjLE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jetl-IOsztfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5\n",
        "\n",
        " Try using dropout, and observe the results."
      ],
      "metadata": {
        "id": "0GhOzLnLzt2U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VhlomecKzv9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c7oc-4iMzwZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6\n",
        "\n",
        "Use the data to predict the gender of the speaker. Try including the formant bandwidths as features as well (\"B1\"-\"B4\")"
      ],
      "metadata": {
        "id": "WGutl91kdwge"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kUZIlAK3eA6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part B: Regression\n",
        "\n",
        "Regression analysis is used to estimate/measure the relationship between an _independent_ variable, say $x$, and a _dependent_ variable, say $y$. One of the simplest regression problems is\n",
        "\\begin{equation}\n",
        "y = ax + b\n",
        "\\end{equation}\n",
        "where $a$ and $b$ are constants. In practice our observations will be contaminated by noise, so we have\n",
        "\\begin{equation}\n",
        "y = ax + b + n,\n",
        "\\end{equation}\n",
        "where $n$ is noise, eg. measurement errors. This particular problem is called _linear regression_.\n",
        "\n",
        "We will have a look at _non-linear regression_, using deep neural networks. Here we are looking at general regression problems in the form\n",
        "\\begin{equation}\n",
        "y = f(x) + n.\n",
        "\\end{equation}\n",
        "\n",
        "We generate our data according to the function $f(x) = x^2 + \\cos(20x) \\text{ sign}(x)$, obtaining a set of observations $\\{(x_i,y_i)\\}$.\n",
        "\n",
        "Then we assume we do not know the underlying function and we try to recover and approximation of $f$ only using the observations $\\{(x_i,y_i)\\}$."
      ],
      "metadata": {
        "id": "g0K3fhPVCCEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define function\n",
        "def f(x):\n",
        "    return x**2 + np.cos(20*x)*np.sign(x)\n",
        "\n",
        "# Setup some simulation parameters\n",
        "# Number of observations\n",
        "N = 5000\n",
        "\n",
        "# Plot a \"clean\" version of the relationship between x and y\n",
        "plt.figure(figsize=(10, 8))\n",
        "x = np.linspace(-2,2,N)\n",
        "plt.plot(x,f(x))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n-tBPqrtCDfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a noise version of the observations\n",
        "y = f(x) + np.random.randn(len(x))\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(x,y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1G9CBawBCWjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to perform regression is to assume that the data is generated using a set of functions from a cerain family, for example polynomials of order $p$,\n",
        "\\begin{equation}\n",
        "\\hat f(x) = a_0 + a_1 x + a_2 x^2 \\ldots a_p x^p.\n",
        "\\end{equation}\n",
        "Then regression corresponds to fitting the parameters in the model. Let us see how this works out before using our neural networks."
      ],
      "metadata": {
        "id": "Im4kKuaMgwAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Give a set of polynomial orders to try\n",
        "P = [1, 2, 5, 10, 20]\n",
        "\n",
        "# Define estimator function. Arguments are inout variable, observation and polynomial order\n",
        "# Returns a set of polynomial coefficients\n",
        "def reg_estimator(x,y,p):\n",
        "    # Use simple ls approach\n",
        "    N = len(x)\n",
        "    H = np.zeros((N,p+1))\n",
        "    for col in range(p+1):\n",
        "        H[:,col] = x**col\n",
        "    iHtH = np.linalg.inv(np.dot(H.T,H))\n",
        "    theta = np.dot(np.dot(iHtH,H.T),y)\n",
        "    return theta\n",
        "\n",
        "# Computes fx) = c_0 + c_1x + c_2 x^2 ... c_p x^p\n",
        "def poly(x, C):\n",
        "    # compute p(x) for coeffs in c\n",
        "    y = 0*x\n",
        "    for p, c in enumerate(C):\n",
        "        y += c*x**p\n",
        "    return y\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(x,f(x),label=\"Truth\")\n",
        "for p in P:\n",
        "    C = reg_estimator(x,y,p)\n",
        "    plt.plot(x,poly(x,C),label=\"Poly order \" + str(p))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fICArE_jCc2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 8\n",
        "\n",
        "Play with different $p$ to see how close you can get to the true function.\n",
        "\n",
        "Note: Very high $p$ will give numerical problems."
      ],
      "metadata": {
        "id": "d18_npjhg605"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "esCdcgsghAMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In what follows we will use a deep neural network to approximate $f$.\n",
        "\n",
        "We train the network by using $x$ as an input and the squared error between the network output $\\hat y$ and the observed value $y$ as a loss\n",
        "\\begin{equation}\n",
        " L = \\frac{1}{N} \\sum_n (\\hat y - y)^2\n",
        "\\end{equation}\n",
        "\n",
        "We first try our network on clean data to check if it works."
      ],
      "metadata": {
        "id": "-ZITwHQRhL_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class CustomSineDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        x: np.array,\n",
        "        y: np.array\n",
        "    ):\n",
        "        self.x = torch.from_numpy(x).unsqueeze(1).float()\n",
        "        self.y = torch.from_numpy(y).unsqueeze(1).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.y[idx]\n",
        "        data = self.x[idx]\n",
        "        return data, label\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer, verbose=False):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if verbose:\n",
        "          if batch % 100 == 0:\n",
        "              loss, current = loss.item(), (batch + 1) * len(X)\n",
        "              print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn, verbose=False):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    if verbose:\n",
        "      print(f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return correct, test_loss\n",
        "\n",
        "x = np.linspace(-2,2,N)\n",
        "train_dataset = CustomSineDataset(x, f(x))\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "### Set up the model\n",
        "epochs = 100\n",
        "\n",
        "model = NeuralNetwork(input_dim=1).to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "validation_results = []\n",
        "\n",
        "### Do the training\n",
        "for t in tqdm(range(epochs), desc='Epochs'):\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    train_corr, train_loss = test(train_dataloader, model, loss_fn)\n",
        "    validation_results.append([t+1, 'accuracy', train_corr, 'train'])\n",
        "    validation_results.append([t+1, 'loss', train_loss, 'train'])\n",
        "\n",
        "### Plot training and validation accuracy and lost across epochs\n",
        "sns.lineplot(\n",
        "    pd.DataFrame(validation_results, columns=['epoch', 'metric', 'value', 'data_type']),\n",
        "    x = 'epoch',\n",
        "    y = 'value',\n",
        "    hue = 'metric',\n",
        "    style = 'data_type'\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "with torch.no_grad():\n",
        "  model_y = model(torch.from_numpy(x).unsqueeze(1).float())\n",
        "\n",
        "df_vals = [ent for ent in zip(x, f(x), ['true'] * len(x))]\n",
        "df_vals.extend([ent for ent in zip(x, model_y.squeeze().numpy(), ['pred'] * len(x))])\n",
        "\n",
        "sns.lineplot(\n",
        "    pd.DataFrame([{'x' : x_val, 'y' : y_val, 'source' : source} for x_val, y_val, source in df_vals]),\n",
        "    x = 'x',\n",
        "    y = 'y',\n",
        "    hue = 'source'\n",
        ")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FOy6a2G2hVr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 9\n",
        "\n",
        "Try increasing the number of nodes in the network to see if the results can be improved."
      ],
      "metadata": {
        "id": "5SiyZNrRhWJY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i12hYYxn6AcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will use a deep network with more than one hidden layer."
      ],
      "metadata": {
        "id": "VUPbwyN458hL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "x = np.linspace(-2,2,N)\n",
        "train_dataset = CustomSineDataset(x, f(x))\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "### Set up the model\n",
        "epochs = 100\n",
        "\n",
        "model = NeuralNetwork(input_dim=1).to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "validation_results = []\n",
        "\n",
        "### Do the training\n",
        "for t in tqdm(range(epochs), desc='Epochs'):\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    train_corr, train_loss = test(train_dataloader, model, loss_fn)\n",
        "    validation_results.append([t+1, 'accuracy', train_corr, 'train'])\n",
        "    validation_results.append([t+1, 'loss', train_loss, 'train'])\n",
        "\n",
        "with torch.no_grad():\n",
        "  model_y = model(torch.from_numpy(x).unsqueeze(1).float())\n",
        "\n",
        "df_vals = [ent for ent in zip(x, f(x), ['true'] * len(x))]\n",
        "df_vals.extend([ent for ent in zip(x, model_y.squeeze().numpy(), ['pred'] * len(x))])\n",
        "\n",
        "sns.lineplot(\n",
        "    pd.DataFrame([{'x' : x_val, 'y' : y_val, 'source' : source} for x_val, y_val, source in df_vals]),\n",
        "    x = 'x',\n",
        "    y = 'y',\n",
        "    hue = 'source'\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EL8NByjF6B2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 10\n",
        "\n",
        "Try increasing the number of hidden nodes per layer until performance is satisfactory. Can the same effect be achieved by just adding more layers?"
      ],
      "metadata": {
        "id": "OqWz-KB46aIS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y1ZMhZfX6ehr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 11\n",
        "\n",
        "Using the best setup from the previous problem, train a model using the noisy data."
      ],
      "metadata": {
        "id": "Ef0BtcaW6fhz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_pMhvWyN6i05"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}