{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TTT4185 Machine learning for Speech technology\n",
        "\n",
        "# Computer assignment 2: Classification using the Bayes Decision Rule and Support Vector Machines\n",
        "\n",
        "This assignment assumes that the student has knowledge about the Bayes Decision Rule, maximum likelihood estimation and support vector machines.\n",
        "\n",
        "In this assignment we will use `scikit-learn` (http://scikit-learn.org/stable/), which is a powerful and very popular Python toolkit for data analysis and machine learning, and `pandas` (https://pandas.pydata.org), which implements the all-powerful `DataFrame`.\n",
        "\n",
        "We will also be using a small database of phonemes, where each phoneme is represented by the four first formant positions (\"F1\"-\"F4\") and their corresponding bandwidths (\"B1\"-\"B4\"). All numbers are in kHz. In addition, the speaker ID and the gender of the speaker are given for each phoneme."
      ],
      "metadata": {
        "id": "gJ5NkK1ydEQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this will fetch the Train.csv and Test.csv so you do not need to manually upload them. Just run this block for each session\n",
        "\n",
        "! wget https://folk.ntnu.no/plparson/ttt4185_data/Train.csv\n",
        "! wget https://folk.ntnu.no/plparson/ttt4185_data/Test.csv"
      ],
      "metadata": {
        "id": "KcDavNHSFYJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: Maximum Likelihood\n",
        "\n",
        "In this problem we will use the Bayes decision rule to classify vowels based on their formants. The formants have been extracted from the open database `VTR Formants database` (http://www.seas.ucla.edu/spapl/VTRFormants.html) created by Microsoft and UCLA.\n",
        "\n"
      ],
      "metadata": {
        "id": "TWMxr-gQdJdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1\n",
        "\n",
        "Download the files `Train.csv` and `Test.csv` from Blackboard then upload them to your Colab workspace. You can load them into a `pandas` dataframe using the command `pd.read_csv`. Using the training data, create a single scatter plot of \"F1\" vs \"F2\" for the three vowels\n",
        "- \"ae\" as in \"bat\"\n",
        "- \"ey\" as in \"bait\"\n",
        "- \"ux\" as in \"boot\"\n",
        "\n",
        "Just eyeing the plots, discuss which classes will be hardest to classify correctly."
      ],
      "metadata": {
        "id": "HC0wNMWGfyFM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWGPiIdTa1Ma"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"Train.csv\")\n",
        "test = pd.read_csv(\"Test.csv\")\n",
        "\n",
        "# Extract vowels\n",
        "train = train[train['Phoneme'].isin(['ae', 'ey', 'ux'])]\n",
        "test = test[test['Phoneme'].isin(['ae', 'ey', 'ux'])]\n",
        "\n",
        "# Plotting here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2\n",
        "\n",
        " Use the Bayes Decision Rule to create a classifier for the phonemes 'ae', 'ey' and 'ux' under the following constraints:\n",
        "- The feature vector $x$ contains the first two formants, \"F1\" and \"F2\".\n",
        "- The distribution of $x$ given a phoneme $c$, $P(x|c)$, is Gaussian.\n",
        "- Use the maximum likelihood estimator to estimate the model parameters."
      ],
      "metadata": {
        "id": "QpVPQSCdfuNy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fi24I3SAODxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3\n",
        "\n",
        "To visualize the classes models and the classifier created in Task 2, plot the contours for each Gaussian distribution in the model. That is, the class conditional likelihoods $P(x|c)$, by using the following function."
      ],
      "metadata": {
        "id": "eFUrxUEzV5i8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats\n",
        "\n",
        "def plotGaussian(mean, cov, color, ax):\n",
        "    \"\"\"\n",
        "        Creates a contour plot for a bi-variate normal distribution\n",
        "\n",
        "        mean: numpy array 2x1 with mean vector\n",
        "        cov: numpy array 2x2 with covarince matrix\n",
        "        color: name of color for the plot (see https://matplotlib.org/stable/gallery/color/named_colors.html)\n",
        "        ax: axis handle where the plot is drawn (can for example be returned by plt.gca() or plt.subplots())\n",
        "    \"\"\"\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    x, y = np.mgrid[xlim[0]:xlim[1]:(xlim[1]-xlim[0])/500.0, ylim[0]:ylim[1]:(ylim[1]-ylim[0])/500.0]\n",
        "    xy = np.dstack((x, y))\n",
        "    mvn = scipy.stats.multivariate_normal(mean, cov)\n",
        "    lik = mvn.pdf(xy)\n",
        "    ax.contour(x,y,lik,colors=color)"
      ],
      "metadata": {
        "id": "-kPDoqF7WOmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optional\n",
        "\n",
        "*Try:* Plot the decision regions for the Bayesian classifier. Tips: Calculate the posterior for each class, use the `numpy.argmax` function to get the decision regions, and `matplotlib.pyplot.contourf` to plot them."
      ],
      "metadata": {
        "id": "t1GxwG5SWqgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lpv6bWJ3OC7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4\n",
        "\n",
        "Test your classifier on the 'ae', 'ey' and 'ux' phonemes from the test set and present your results in a _confusion matrix_. That is, a table where you see how many times 'ae' was correctly classified, how many times it was wrongly classified as 'ey' and so on."
      ],
      "metadata": {
        "id": "3QRS28PBWk8J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eAFA9IMjOCae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5\n",
        "\n",
        "Extend your classifier to include the features \"F1\"-\"F4\" and compare the results with those in Task 4."
      ],
      "metadata": {
        "id": "qYc4cv8lXE4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xRUQ3gs7OCBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6\n",
        "\n",
        "Finally use all available information \"F1\"-\"F4\" and \"B1-B4\". How does the performance of this classifier compare with the simpler classifiers using fewer features?"
      ],
      "metadata": {
        "id": "SPHM4UTbMNO8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E-JwFRZVOBjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 7\n",
        "\n",
        "To better understand how important each feature is to the model, we will now perform permutation feature importance (https://scikit-learn.org/stable/modules/permutation_importance.html). The idea is that for each feature we will randomly shuffle the values and the ask the trained model to predict using this shuffled data. We can then observe how the performance degregates as each feature is shuffled.\n",
        "\n",
        "For the model, take one of the 8 features (four formants and four bandwidths), shuffle and evaluate it 10 times. Repeat this process for each feature. Which feature has the biggest impact on model performance? Does this align with your knowlege of formants?\n",
        "\n",
        "Below we've provided a sample function to get you started"
      ],
      "metadata": {
        "id": "iXOb7td0XPip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_permutation_eval(classifier, df: pd.DataFrame, feature: str, num_iter: int) -> list:\n",
        "  feat_results = []\n",
        "  shuffle_df = df.copy(deep=True)\n",
        "  # shuffle and evaluate multiple times to ensure the trends are real\n",
        "  for i in range(num_iter):\n",
        "    shuffled_values = list(shuffle_df[feature])\n",
        "    # shuffle does this in-place\n",
        "    np.random.shuffle(shuffled_values)\n",
        "    shuffle_df[feature] = shuffled_values\n",
        "\n",
        "    # This is psuedo-code\n",
        "    # Modify to work with your own classifier implementations\n",
        "    run_results = classifier.do_prediction(shuffle_df)\n",
        "    run_accuracy = calculate_error_rate(run_results)\n",
        "    feat_results.append(run_accuracy)\n",
        "  return feat_results"
      ],
      "metadata": {
        "id": "AdO50afpOBFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 8\n",
        "\n",
        "We want to make the model slightly more powerful by modeling the feature vector conditional on both the vowel and gender of speaker, that is $P(x|g,c)$, where $g$ is the gender of the speaker and $c$ is the phoneme label. Show how these models can be used for phoneme classification using marginalization over the gender.\n",
        "\n",
        "Assume that $P(x|g,c)$ is a multivariate Gaussian and compute the maximum likelihood estimates for the models. Compare the result on the test set with the results in Task 5."
      ],
      "metadata": {
        "id": "kWcgRSDJ_Pwr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHlAjD7m_aF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 9\n",
        "\n",
        "When using Gaussian classifiers we often avoid computing the entire covariance matrix, but instead we only use the diagonal of the matrix. Repeat the results in Task 5 using only diagonal covariance matrices and compare the results."
      ],
      "metadata": {
        "id": "ihCUtXWA_gKw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7urSoetz_q9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: SVMs\n",
        "\n",
        "In this problem we use the support vector machine (SVM) to build classifiers. We use the same dataset as in Problem 1. It is up to you to select which features to use.\n",
        "\n",
        "We use the function `sklearn.svm.SVC` from `scikit-learn` in this problem.\n",
        "\n",
        "An example on how to use the `SVC` is given in http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC. In short, we do the following (for a linear kernel):\n",
        "- Instantiate an SVC object: `cls = SVC(kernel='linear')`\n",
        "- Train the SVM using the feature vector matrix `train_X`, and label vector `train_Y`: `cls.fit(train_X, train_Y)`\n",
        "- Predict labels on the test set `Test_X` using: `cls.predict(Test_X)`\n",
        "\n",
        "You can use or adapt the following functions to visualize the SVM decision regions and support vectors in 2D."
      ],
      "metadata": {
        "id": "8SDx8wmN_qNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def Plot_SVM_decision_regions(clf: SVC, data: np.array, labels: np.array) -> None:\n",
        "    '''\n",
        "    This function is for plotting the decision area of SVM\n",
        "\n",
        "    Args:\n",
        "    - clf: SVM model\n",
        "    - data: Data with two features\n",
        "    - labels: Corresponding labels of the data\n",
        "    '''\n",
        "    x_min, x_max = data[:,0].min() - 0.2, data[:,0].max() + 0.2\n",
        "    y_min, y_max = data[:,1].min() - 0.2, data[:,1].max() + 0.2\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.002),np.arange(y_min, y_max, 0.002))\n",
        "    # we need to use the LabelEncoder here to make contourf work\n",
        "    label_encoder = LabelEncoder()\n",
        "    integer_encoded = label_encoder.fit_transform(labels)\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = label_encoder.transform(Z)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    #Plotting\n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.scatterplot(x = data[:,0], y = data[:,1],hue=labels)\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.ocean, alpha=0.2)\n",
        "    plt.legend()\n",
        "    plt.title('Decision Area of SVM')\n",
        "    plt.show()\n",
        "\n",
        "def Plot_Support_Vectors(clf: SVC, data: np.array) -> None:\n",
        "    '''\n",
        "    This function is for plotting the support vectors of the SVM model\n",
        "\n",
        "    Args:\n",
        "    - clf: SVM model\n",
        "    - data: Data with two features\n",
        "    '''\n",
        "    x_min, x_max = data[:,0].min() - 0.2, data[:,0].max() + 0.2\n",
        "    y_min, y_max = data[:,1].min() - 0.2, data[:,1].max() + 0.2\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.002),np.arange(y_min, y_max, 0.002))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    # we need to use the LabelEncoder here to make contourf work\n",
        "    label_encoder = LabelEncoder()\n",
        "    Z = label_encoder.fit_transform(Z)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    #Plotting\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.scatter(clf.support_vectors_[:,0], clf.support_vectors_[:,1], c='k',alpha=0.4,label='support vector')\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.ocean, alpha=0.2)\n",
        "    plt.legend()\n",
        "    plt.title('Support Vectors')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xrQAQ7LOCbdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 10\n",
        "\n",
        "Create a linear SVM with different penalty terms $C=\\{0.1, 1, 10\\}$ and compare with the results in Problem 1."
      ],
      "metadata": {
        "id": "EZt2L6BVCkyB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eG94C_2lOfgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 11\n",
        "\n",
        "Try different kernels ('rbf', 'poly', 'sigmoid') and compare the results. Choose one of the kernels and use different penalty terms $C$. What happens with the performance on the training set when you increase $C$? What happens with the performance on the test set?"
      ],
      "metadata": {
        "id": "7uU2ipiICpvN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hMClmIDXCpZx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}